{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f06da4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (50000, 32, 32, 3)\n",
      "Training labels shape: (50000,)\n",
      "Test data shape: (10000, 32, 32, 3)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Specify the name of the folder containing CIFAR-10 batches\n",
    "folder_name = 'cifar10_batches'  # Replace with your actual folder name\n",
    "\n",
    "# Construct the path to the folder\n",
    "folder_path = os.path.join(current_dir, folder_name)\n",
    "\n",
    "# Initialize empty lists to store data and labels\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "# Load and concatenate training batches\n",
    "for i in range(1, 6):\n",
    "    batch_file_path = os.path.join(folder_path, f'data_batch_{i}')\n",
    "    with open(batch_file_path, 'rb') as file:\n",
    "        batch_data = pickle.load(file, encoding='bytes')\n",
    "        all_data.append(batch_data[b'data'])\n",
    "        all_labels.extend(batch_data[b'labels'])\n",
    "\n",
    "# Load test batch\n",
    "test_batch_file_path = os.path.join(folder_path, 'test_batch')\n",
    "with open(test_batch_file_path, 'rb') as file:\n",
    "    test_batch_data = pickle.load(file, encoding='bytes')\n",
    "    all_data.append(test_batch_data[b'data'])\n",
    "    all_labels.extend(test_batch_data[b'labels'])\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "x_train_and_test = np.concatenate(all_data, axis=0).astype(float)\n",
    "y_train_and_test = np.array(all_labels).astype(float)\n",
    "\n",
    "# Reshape the data if needed (CIFAR-10 images are 32x32x3)\n",
    "x_train_and_test = x_train_and_test.reshape((len(x_train_and_test), 32, 32, 3))\n",
    "\n",
    "# Split into training and test sets\n",
    "x_train = x_train_and_test[:-len(test_batch_data[b'labels'])]\n",
    "y_train = y_train_and_test[:-len(test_batch_data[b'labels'])]\n",
    "x_test = x_train_and_test[-len(test_batch_data[b'labels']):]\n",
    "y_test = y_train_and_test[-len(test_batch_data[b'labels']):]\n",
    "\n",
    "# Print the shape of the loaded data\n",
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Test data shape:\", x_test.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c05dbd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat = x_train.reshape((len(x_train), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02180e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[154. 126. 105.]\n",
      "  [102. 125. 155.]\n",
      "  [172. 180. 142.]\n",
      "  ...\n",
      "  [ 88. 103.  94.]\n",
      "  [ 65.  83.  90.]\n",
      "  [ 79.  68.  67.]]\n",
      "\n",
      " [[136. 137. 122.]\n",
      "  [132. 151. 181.]\n",
      "  [203. 208. 208.]\n",
      "  ...\n",
      "  [ 92.  88.  78.]\n",
      "  [ 87.  98.  76.]\n",
      "  [ 67.  81.  91.]]\n",
      "\n",
      " [[146. 124.  88.]\n",
      "  [ 85.  87.  84.]\n",
      "  [ 75.  78.  69.]\n",
      "  ...\n",
      "  [169. 113.  89.]\n",
      "  [ 84.  65.  56.]\n",
      "  [ 88.  81.  63.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[158.  83.  55.]\n",
      "  [ 46.  51.  52.]\n",
      "  [ 48.  46.  57.]\n",
      "  ...\n",
      "  [134. 121.  61.]\n",
      "  [ 51.  33.  21.]\n",
      "  [ 53.  51. 107.]]\n",
      "\n",
      " [[172. 166. 123.]\n",
      "  [140. 160. 155.]\n",
      "  [139. 131. 126.]\n",
      "  ...\n",
      "  [ 96. 101. 135.]\n",
      "  [ 87.  78.  29.]\n",
      "  [ 84.  73.  94.]]\n",
      "\n",
      " [[166. 160. 170.]\n",
      "  [163. 165. 171.]\n",
      "  [180. 186. 174.]\n",
      "  ...\n",
      "  [ 42.  67. 101.]\n",
      "  [122. 133. 136.]\n",
      "  [139. 142. 144.]]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d84ce108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of x_train_flat: float64\n",
      "Flattened training data shape: (50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Specify the name of the folder containing CIFAR-10 batches\n",
    "folder_name = 'cifar10_batches'  # Replace with your actual folder name\n",
    "\n",
    "# Construct the path to the folder\n",
    "folder_path = os.path.join(current_dir, folder_name)\n",
    "\n",
    "# Initialize empty lists to store data and labels\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "# Load and concatenate training batches\n",
    "for i in range(1, 6):\n",
    "    batch_file_path = os.path.join(folder_path, f'data_batch_{i}')\n",
    "    with open(batch_file_path, 'rb') as file:\n",
    "        batch_data = pickle.load(file, encoding='bytes')\n",
    "        all_data.append(batch_data[b'data'])\n",
    "        all_labels.extend(batch_data[b'labels'])\n",
    "\n",
    "# Load test batch\n",
    "test_batch_file_path = os.path.join(folder_path, 'test_batch')\n",
    "with open(test_batch_file_path, 'rb') as file:\n",
    "    test_batch_data = pickle.load(file, encoding='bytes')\n",
    "    all_data.append(test_batch_data[b'data'])\n",
    "    all_labels.extend(test_batch_data[b'labels'])\n",
    "\n",
    "# Convert the lists to NumPy arrays and cast to float\n",
    "x_train_and_test = np.concatenate(all_data, axis=0).astype(float)\n",
    "y_train_and_test = np.array(all_labels).astype(float)\n",
    "\n",
    "# Reshape the data if needed (CIFAR-10 images are 32x32x3)\n",
    "x_train_and_test = x_train_and_test.reshape((len(x_train_and_test), 32, 32, 3))\n",
    "\n",
    "# Split into training and test sets\n",
    "x_train = x_train_and_test[:-len(test_batch_data[b'labels'])]\n",
    "y_train = y_train_and_test[:-len(test_batch_data[b'labels'])]\n",
    "x_test = x_train_and_test[-len(test_batch_data[b'labels']):]\n",
    "y_test = y_train_and_test[-len(test_batch_data[b'labels']):]\n",
    "\n",
    "# Flatten each image in the training set\n",
    "x_train_flat = x_train.reshape((len(x_train), -1))  # -1 automatically computes the size of the remaining dimensions\n",
    "\n",
    "# Print the data types and the shape of the loaded and flattened training data\n",
    "print(\"Data type of x_train_flat:\", x_train_flat.dtype)\n",
    "print(\"Flattened training data shape:\", x_train_flat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "077e0656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[154. 126. 105. ... 139. 142. 144.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_flat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01716bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of x_train_flat: float64\n",
      "Normalized and flattened training data shape: (50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Specify the name of the folder containing CIFAR-10 batches\n",
    "folder_name = 'cifar10_batches'  # Replace with your actual folder name\n",
    "\n",
    "# Construct the path to the folder\n",
    "folder_path = os.path.join(current_dir, folder_name)\n",
    "\n",
    "# Initialize empty lists to store data and labels\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "# Load and concatenate training batches\n",
    "for i in range(1, 6):\n",
    "    batch_file_path = os.path.join(folder_path, f'data_batch_{i}')\n",
    "    with open(batch_file_path, 'rb') as file:\n",
    "        batch_data = pickle.load(file, encoding='bytes')\n",
    "        all_data.append(batch_data[b'data'])\n",
    "        all_labels.extend(batch_data[b'labels'])\n",
    "\n",
    "# Load test batch\n",
    "test_batch_file_path = os.path.join(folder_path, 'test_batch')\n",
    "with open(test_batch_file_path, 'rb') as file:\n",
    "    test_batch_data = pickle.load(file, encoding='bytes')\n",
    "    all_data.append(test_batch_data[b'data'])\n",
    "    all_labels.extend(test_batch_data[b'labels'])\n",
    "\n",
    "# Convert the lists to NumPy arrays, cast to float, and normalize\n",
    "x_train_and_test = np.concatenate(all_data, axis=0).astype(float) / 255.0\n",
    "y_train_and_test = np.array(all_labels).astype(float)\n",
    "\n",
    "# Reshape the data if needed (CIFAR-10 images are 32x32x3)\n",
    "x_train_and_test = x_train_and_test.reshape((len(x_train_and_test), 32, 32, 3))\n",
    "\n",
    "# Split into training and test sets\n",
    "x_train = x_train_and_test[:-len(test_batch_data[b'labels'])]\n",
    "y_train = y_train_and_test[:-len(test_batch_data[b'labels'])]\n",
    "x_test = x_train_and_test[-len(test_batch_data[b'labels']):]\n",
    "y_test = y_train_and_test[-len(test_batch_data[b'labels']):]\n",
    "\n",
    "# Flatten each image in the training set\n",
    "x_train_flat = x_train.reshape((len(x_train), -1))  # -1 automatically computes the size of the remaining dimensions\n",
    "\n",
    "# Print the data types and the shape of the loaded, normalized, and flattened training data\n",
    "print(\"Data type of x_train_flat:\", x_train_flat.dtype)\n",
    "print(\"Normalized and flattened training data shape:\", x_train_flat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1873998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60392157 0.49411765 0.41176471 ... 0.54509804 0.55686275 0.56470588]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_flat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c2729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
